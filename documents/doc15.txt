Support vector machines and
machine learning on documents
Improving classifier effectiveness has been an area of intensive machinelearning
research over the last two decades, and this work has led to a new
generation of state-of-the-art classifiers, such as support vector machines,
boosted decision trees, regularized logistic regression, neural networks, and
random forests. Many of these methods, including support vector machines
(SVMs), the main topic of this chapter, have been applied with success to
information retrieval problems, particularly text classification. An SVM is a
kind of large-margin classifier: it is a vector space based machine learning
method where the goal is to find a decision boundary between two classes
that is maximally far from any point in the training data (possibly discounting
some points as outliers or noise).
We will initially motivate and develop SVMs for the case of two-class data
sets that are separable by a linear classifier (Section 15.1), and then extend the
model in Section 15.2 to non-separable data, multi-class problems, and nonlinear
models, and also present some additional discussion of SVM performance.
The chapter then moves to consider the practical deployment of text
classifiers in Section 15.3: what sorts of classifiers are appropriate when, and
how can you exploit domain-specific text features in classification? Finally,
we will consider how the machine learning technology that we have been
building for text classification can be applied back to the problemof learning
how to rank documents in ad hoc retrieval (Section 15.4). While several machine
learning methods have been applied to this task, use of SVMs has been
prominent. Support vector machines are not necessarily better than other
machine learning methods (except perhaps in situations with little training
data), but they perform at the state-of-